# SQS/DynamoDB Backend Implementation Plan

## Overview

This document outlines the implementation plan for adding production-ready AWS backend support to airunner, using SQS for job queueing and DynamoDB for persistence. The design maintains full API compatibility with the existing `JobStore` interface.

## Architecture Summary

```
┌─────────────────────────────────────────────────────────────────────────┐
│                           SQSJobStore                                   │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│   ┌──────────────┐    ┌──────────────┐    ┌──────────────┐              │
│   │     SQS      │    │   DynamoDB   │    │   DynamoDB   │              │
│   │   Queues     │    │  Jobs Table  │    │ Events Table │              │
│   ├──────────────┤    ├──────────────┤    ├──────────────┤              │
│   │ • Dequeue    │    │ • Job state  │    │ • Event log  │              │
│   │ • Visibility │    │ • Metadata   │    │ • Streaming  │              │
│   │ • Delete     │    │ • Idempotency│    │ • Replay     │              │
│   └──────────────┘    └──────────────┘    └──────────────┘              │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

**Key Design Principle:** SQS is transport-only; DynamoDB is the source of truth.

## AWS Resources

### SQS Queues

One SQS Standard queue per logical job queue:

- **Naming**: `airunner-{env}-{queueName}` (e.g., `airunner-prod-default`)
- **Configuration**:
  - `VisibilityTimeout`: 300 seconds (default, overridden per-message)
  - `ReceiveMessageWaitTimeSeconds`: 20 (long polling)
  - `MessageRetentionPeriod`: 14 days
  - Dead-letter queue with redrive policy (maxReceiveCount: 3)
  - SSE-SQS encryption enabled

**SQS Message Body Format:**

```json
{
  "job_id": "01234567-89ab-cdef-0123-456789abcdef",
  "queue": "default",
  "attempt": 1
}
```

### DynamoDB Tables

#### Jobs Table: `airunner_jobs`

| Attribute | Type | Description |
|-----------|------|-------------|
| `job_id` (PK) | S | UUIDv7 job identifier |
| `queue` | S | Queue name |
| `state` | N | JobState enum value |
| `job_params` | M | Serialized JobParams |
| `request_id` | S | Idempotency token |
| `created_at` | N | Unix milliseconds |
| `updated_at` | N | Unix milliseconds |
| `result` | M | JobResult (populated on completion) |

**Global Secondary Indexes:**

| Index | PK | SK | Projection | Purpose |
|-------|----|----|------------|---------|
| GSI1 | `queue` | `created_at` | ALL | ListJobs by queue |
| GSI2 | `request_id` | - | KEYS_ONLY | Idempotency lookup |

**Capacity:** On-demand billing mode for unpredictable workloads.

#### JobEvents Table: `airunner_job_events`

| Attribute | Type | Description |
|-----------|------|-------------|
| `job_id` (PK) | S | Job identifier |
| `sequence` (SK) | N | Monotonic sequence number |
| `timestamp` | N | Unix milliseconds |
| `event_type` | N | EventType enum value |
| `event_payload` | B | Protobuf-encoded event data |
| `ttl` | N | TTL attribute (Unix seconds) |

**Configuration:**
- TTL enabled on `ttl` attribute (recommended: 7-30 days retention)
- DynamoDB Streams: NEW_IMAGE (for cross-instance fanout, optional)

## Task Token Design

Use a **stateless, self-describing task token** for cross-instance compatibility:

```
taskToken = base64url(job_id + "|" + queue_name + "|" + sqs_receipt_handle)
```

Benefits:
- No in-memory state required
- Works across multiple API server replicas
- Self-contained: any server can process any token
- Still opaque to clients

## Event Sequencing

**Client-Side Sequence Generation**: Sequence numbers are generated by the worker (client) and must be monotonic within the scope of a single job. The server accepts and stores client-provided sequences without modification.

**Design Rationale:**
- Single-worker-per-job guarantee (enforced via SQS visibility timeout) eliminates sequence conflicts
- Worker knows the true event order as work happens
- Simpler server implementation with no coordination overhead
- Idempotent retries: same events with same sequences can be safely re-published

**Sequence Scope**: Sequences are per-job, starting from 0 or 1 for each job. Sequences across different jobs are independent.

**Event Payload Format**: The `event_payload` attribute stores the **entire JobEvent message** serialized as protobuf binary format, including sequence, timestamp, event_type, and event_data.

**Worker Responsibilities:**
- Generate monotonically increasing sequence numbers for events within a job
- Start sequences at 0 or 1 for each new job
- Maintain sequence consistency across event publishing retries

## Implementation Details

### SQSJobStore Structure

```go
type SQSJobStoreConfig struct {
    QueueURLs                       map[string]string // queue name -> SQS URL
    JobsTableName                   string
    JobEventsTableName              string
    DefaultVisibilityTimeoutSeconds int32
    EventsTTLDays                   int32 // Optional: TTL for event retention (0 = no TTL)
}

type SQSJobStore struct {
    sqsClient    *sqs.Client
    dynamoClient *dynamodb.Client
    cfg          SQSJobStoreConfig

    // Local event streaming (same semantics as MemoryJobStore)
    mu           sync.RWMutex
    eventStreams map[string][]chan *jobv1.JobEvent

    stopCh chan struct{}
    wg     sync.WaitGroup
}
```

### Method Implementations

#### EnqueueJob

1. **Idempotency check**: Query GSI2 by `request_id`
   - If exists, return existing job metadata
2. **Create job in DynamoDB**: PutItem with `attribute_not_exists(job_id)` condition
3. **Send to SQS**: Minimal JSON message body
4. **Return**: `EnqueueJobResponse{JobId, CreatedAt, State}`

```go
func (s *SQSJobStore) EnqueueJob(ctx context.Context, req *jobv1.EnqueueJobRequest) (*jobv1.EnqueueJobResponse, error) {
    // 1. Check idempotency via GSI2
    existing, err := s.getJobByRequestID(ctx, req.RequestId)
    if err != nil {
        return nil, err
    }
    if existing != nil {
        return &jobv1.EnqueueJobResponse{
            JobId:     existing.JobId,
            CreatedAt: existing.CreatedAt,
            State:     existing.State,
        }, nil
    }

    // 2. Create job record
    jobId := uuid.Must(uuid.NewV7()).String()
    now := time.Now()
    
    // PutItem to DynamoDB...
    
    // 3. Send to SQS
    // SendMessage with job_id in body...
    
    return &jobv1.EnqueueJobResponse{...}, nil
}
```

#### DequeueJobs

1. **ReceiveMessage** from SQS (max 10 messages per call)
2. **Get job metadata** from DynamoDB for each message
3. **Filter poison messages**: Skip if job not found or already completed
4. **Update job state** to RUNNING in DynamoDB
5. **Set visibility timeout** via ChangeMessageVisibility
6. **Build task tokens** from job_id + queue + receipt_handle
7. **Return** slice of `JobWithToken`

```go
func (s *SQSJobStore) DequeueJobs(ctx context.Context, queue string, maxJobs int, timeoutSeconds int) ([]*JobWithToken, error) {
    queueURL := s.cfg.QueueURLs[queue]
    n := min(maxJobs, 10) // SQS limit
    
    output, err := s.sqsClient.ReceiveMessage(ctx, &sqs.ReceiveMessageInput{
        QueueUrl:            aws.String(queueURL),
        MaxNumberOfMessages: int32(n),
        WaitTimeSeconds:     0, // Long polling handled at service layer
    })
    // Process messages...
}
```

#### UpdateJobVisibility

1. **Decode task token** → job_id, queue, receipt_handle
2. **Validate queue** matches embedded queue name
3. **ChangeMessageVisibility** in SQS
4. **Update updated_at** in DynamoDB (optional)

#### CompleteJob

1. **Decode task token** → job_id, queue, receipt_handle
2. **Validate job_id** matches result.job_id
3. **UpdateItem** in DynamoDB: set state, result fields
4. **DeleteMessage** from SQS
5. **Handle race conditions**: Job may reappear if delete fails; guard on dequeue

#### ListJobs

1. **Query GSI1** if queue specified, otherwise **Scan** (with warning)
2. **Apply FilterExpression** for state
3. **In-memory pagination** using page/page_size
4. **Return** jobs and last_page

#### PublishEvents

1. **Decode task token** → job_id
2. **Validate job exists** in DynamoDB
3. **Accept client-provided sequences**: Events arrive with sequence numbers already set by the worker
4. **Set timestamps**: If event timestamp is not set, use server time
5. **BatchWriteItem** events to JobEvents table
   - Chunk into batches of 25 items (DynamoDB BatchWriteItem limit)
   - Marshal entire JobEvent to protobuf binary for `event_payload`
   - Use client-provided `event.Sequence` as DynamoDB sort key
   - Calculate and set `ttl` attribute if EventsTTLDays is configured
   - Store `timestamp` and `event_type` as separate attributes for efficient querying
6. **Handle partial failures**: Retry unprocessed items returned by BatchWriteItem
7. **Local fanout** to active streams via `fanoutEvent()`

**Error Handling:**
- Return error if job doesn't exist
- Return error if event marshaling fails (fail-fast on data corruption)
- **Persistence failures**: Return error if DynamoDB BatchWriteItem fails after retries
  - Retries unprocessed items with exponential backoff (up to 3 attempts)
  - Detects retryable AWS errors (throttling, service unavailable)
  - Returns error to caller on persistent failure
- **Local fanout**: Always attempted, even if persistence fails (availability over consistency for real-time streaming)
- **Metrics**: Records publish attempts, errors, duration, and persistence success/failure

**Event Criticality (Worker Perspective):**

Workers classify events by criticality when publishing:
- **Critical Events** (ProcessStart, ProcessEnd): Worker fails job if publishing fails
  - Rationale: These structural events are essential for job lifecycle tracking
  - Impact: Communication failure prevents job completion
- **Best-Effort Events** (OutputData): Worker logs warning and continues if publishing fails
  - Rationale: Losing individual output lines shouldn't terminate job execution
  - Impact: Partial output delivered to client
- **Error Events** (ProcessError): Best-effort when published during error handling
  - Rationale: Already in failure state, don't mask original error
  - Impact: Original error always returned to caller

This two-level error handling strategy (server persistence + worker criticality) balances reliability with availability.

#### StreamEvents

1. **Validate job exists** in DynamoDB
2. **Create channel** and register in eventStreams map
3. **Spawn goroutine** to handle historical replay and real-time streaming:
   - **Query historical events** from JobEvents table by `job_id` (returns sorted by `sequence`)
   - **Apply filters**:
     - `fromSequence`: Skip events with `sequence < fromSequence`
     - `fromTimestamp`: Skip events with `timestamp < fromTimestamp`
     - `eventFilter`: Include only specified event types (empty = all types)
   - **Unmarshal event_payload**: Deserialize protobuf binary to JobEvent
   - **Send historical events** to channel in sequence order
   - **Switch to real-time mode**: Wait for context cancellation while receiving events via fanoutEvent
4. **Real-time events** arrive via PublishEvents → fanoutEvent
5. **Cleanup on context cancel**: Remove channel from eventStreams map and close channel

**Implementation Notes:**
- Use DynamoDB Query with KeyConditionExpression on `job_id`
- Results are automatically sorted by `sequence` (sort key)
- Handle pagination if job has many events (use LastEvaluatedKey)
- Non-blocking sends to prevent slow consumers from blocking PublishEvents

## Error Handling

| Scenario | Error Code | Handling |
|----------|------------|----------|
| Invalid task token | `InvalidArgument` | Return immediately |
| Job not found | `NotFound` | Return error |
| Queue not configured | `InvalidArgument` | Return error |
| AWS throttling | `ResourceExhausted` | Retry with backoff |
| AWS internal error | `Internal` | Log and return |
| Poison message | N/A | Delete from SQS, skip |

## Operational Considerations

### At-Least-Once Processing and Single-Worker Guarantee

**Single Worker Per Job**: The system guarantees that only one worker processes a job at any given time through:
- **SQS visibility timeout**: Dequeued jobs become invisible to other workers for the timeout duration
- **HMAC-signed task tokens**: Bind a specific worker to a job, preventing interference from other workers
- **Job state validation**: Already-completed jobs (COMPLETED/FAILED) are filtered out on dequeue

This single-worker guarantee is critical for the event sequencing design, as it eliminates the need for distributed coordination of sequence numbers. Each worker can safely generate monotonic sequences knowing no other worker is concurrently processing the same job.

**At-Least-Once Delivery**: SQS provides at-least-once delivery. Handle redelivery:

1. **On dequeue**: Check job state in DynamoDB
   - If COMPLETED/FAILED: delete SQS message, skip
2. **Workers must be idempotent**: Same job_id may be processed multiple times if:
   - Visibility timeout expires before job completes
   - CompleteJob succeeds but DeleteMessage fails
3. **Event idempotency**: Workers should use the same sequence numbers if re-publishing events for the same work to avoid duplicates in the event stream

### Race Conditions

**CompleteJob vs. Visibility Timeout:**
- If CompleteJob updates DynamoDB but fails to delete SQS message, job may reappear
- Guard: Check job state on dequeue before returning to worker

### Cost Optimization

- Use on-demand DynamoDB billing for variable workloads
- Enable TTL on JobEvents to auto-expire old events
- Use SQS long polling (20s) to reduce empty receives
- Consider S3 for large event payloads (OutputEvent.output)

### Monitoring

The system emits comprehensive OpenTelemetry metrics and traces for observability. Telemetry is configured via environment variables and exports to OTLP-compatible backends (Honeycomb, Datadog, Grafana Cloud, etc.).

#### OpenTelemetry Configuration

Set the following environment variables:

```bash
# OTLP endpoint (e.g., Honeycomb, Grafana Cloud)
OTEL_EXPORTER_OTLP_ENDPOINT=https://api.honeycomb.io

# Authentication headers
OTEL_EXPORTER_OTLP_HEADERS=x-honeycomb-team=YOUR_API_KEY

# Service identification (optional, overrides default)
OTEL_SERVICE_NAME=airunner-server

# Additional resource attributes (optional)
OTEL_RESOURCE_ATTRIBUTES=deployment.environment=production,service.version=1.0.0
```

#### Metrics Reference

**Event Metrics:**
- `airunner.events.publish.total` (counter): Total event publish attempts
  - Attributes: `job_id`, `success` (bool)
- `airunner.events.publish.errors.total` (counter): Event publish errors
  - Attributes: `job_id`, `error_type`
- `airunner.events.publish.duration` (histogram): Event publish duration in milliseconds
  - Attributes: `job_id`
- `airunner.events.persisted.total` (counter): Events successfully persisted to DynamoDB
  - Attributes: `job_id`
- `airunner.events.dropped.total` (counter): Events dropped due to errors or overflow
  - Attributes: `job_id`, `reason`

**Stream Metrics:**
- `airunner.streams.active` (up-down counter): Number of active event streams
- `airunner.streams.historical_replay.duration` (histogram): Historical replay duration in milliseconds
- `airunner.streams.historical_replay.events.total` (counter): Historical events replayed

**Store Operation Metrics:**
- `airunner.jobs.enqueued.total` (counter): Jobs enqueued
  - Attributes: `queue`
- `airunner.jobs.dequeued.total` (counter): Jobs dequeued
  - Attributes: `queue`
- `airunner.jobs.completed.total` (counter): Jobs completed
  - Attributes: `queue`, `state` (COMPLETED/FAILED)
- `airunner.jobs.visibility_updates.total` (counter): Visibility timeout updates

**DynamoDB Metrics:**
- `airunner.dynamodb.operations.total` (counter): DynamoDB operations
  - Attributes: `operation` (PutItem, Query, BatchWriteItem, etc.)
- `airunner.dynamodb.throttles.total` (counter): DynamoDB throttling events
  - Attributes: `operation`
- `airunner.dynamodb.batch_retries.total` (counter): Batch write retries

**Channel Metrics:**
- `airunner.channels.overflow.total` (counter): Channel overflow events (data loss indicator)
  - Attributes: `job_id`

#### Traces

Distributed tracing is enabled for all gRPC operations using OpenTelemetry's W3C TraceContext propagation. Traces include:
- Job lifecycle spans (enqueue → dequeue → execute → complete)
- DynamoDB operations with request/response sizes
- SQS operations with queue names and message counts
- Event publishing with batch sizes

#### Alerting Recommendations

Monitor these metrics for operational health:

1. **Event Loss**: Alert if `airunner.events.dropped.total` or `airunner.channels.overflow.total` increases
2. **Persistence Failures**: Alert if `airunner.events.publish.errors.total` with `error_type=persistence_failed` is sustained
3. **DynamoDB Throttling**: Alert if `airunner.dynamodb.throttles.total` rate exceeds acceptable threshold
4. **Channel Overflow**: Alert on any `airunner.channels.overflow.total` events (indicates slow consumers)
5. **Job Completion Rate**: Alert if `airunner.jobs.completed.total{state=FAILED}` ratio increases

#### Implementation

Telemetry is implemented in `internal/telemetry/`:
- `telemetry.go`: OTLP exporter initialization for traces and metrics
- `metrics.go`: Metric instrument definitions

The server initializes telemetry on startup with graceful degradation if the OTLP endpoint is unreachable.

## Implementation Phases

### Phase 1: Core Store Implementation (2-3 days)

- [x] Create `internal/store/sqs_store.go`
- [x] Implement `SQSJobStore` struct and config
- [x] Implement `EnqueueJob` with idempotency
- [x] Implement `DequeueJobs` with visibility management
- [x] Implement `UpdateJobVisibility`
- [x] Implement `CompleteJob`
- [x] Implement `ListJobs` via GSI1
- [x] Add task token encoding/decoding utilities

### Phase 2: Event Streaming (Completed December 20, 2025)

- [x] Implement `PublishEvents` with DynamoDB writes
- [x] Implement `StreamEvents` with historical replay
- [x] Implement local `fanoutEvent` for real-time streaming
- [x] Add TTL support for event retention
- [x] Add comprehensive integration tests
- [x] Add protobuf marshaling utilities

### Phase 3: Infrastructure & Testing (1-2 days)

- [ ] Create Terraform/CDK for AWS resources
- [ ] Add integration tests with LocalStack
- [ ] Add configuration loading for AWS clients
- [ ] Update orchestrator cmd to use SQSJobStore
- [ ] Document deployment and configuration

### Phase 4: Production Hardening

- [x] Add structured logging with zerolog
- [x] Add metrics emission (OpenTelemetry)
- [x] Add distributed tracing (OpenTelemetry)
- [x] Implement retry policies with exponential backoff
- [x] Add retryable AWS error detection
- [ ] Add health checks for AWS connectivity
- [ ] Load testing and capacity planning

## Future Enhancements

### Multi-Instance Event Streaming

For cross-instance real-time streaming:

1. Enable DynamoDB Streams on JobEvents table
2. Add KCL-based stream consumer to each API server
3. Demux events by job_id and fanout to local subscribers

### Large Payload Offloading

For jobs with large parameters or output:

1. Store payloads in S3 with key format: `jobs/{job_id}/params.json`
2. Store S3 references in DynamoDB
3. Generate pre-signed URLs for direct client access

### Cursor-Based Pagination

Replace page/page_size with cursor-based pagination:

1. Use DynamoDB `LastEvaluatedKey` as cursor
2. Base64-encode cursor for API response
3. Maintain backwards compatibility with page-based API

## Configuration Example

### Application Configuration (YAML)

```yaml
store:
  type: sqs
  aws:
    region: us-west-2
  sqs:
    queues:
      default: https://sqs.us-west-2.amazonaws.com/123456789/airunner-prod-default
      priority: https://sqs.us-west-2.amazonaws.com/123456789/airunner-prod-priority
    default_visibility_timeout_seconds: 300
  dynamodb:
    jobs_table: airunner_jobs
    events_table: airunner_job_events
    events_ttl_days: 30  # Event retention period (0 = no TTL)
```

### Environment Variables

**AWS Configuration:**
```bash
AWS_REGION=us-west-2
AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE
AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
```

**OpenTelemetry Configuration:**
```bash
# OTLP endpoint (required for telemetry)
OTEL_EXPORTER_OTLP_ENDPOINT=https://api.honeycomb.io

# Authentication (varies by provider)
OTEL_EXPORTER_OTLP_HEADERS=x-honeycomb-team=YOUR_API_KEY

# Service identification (optional)
OTEL_SERVICE_NAME=airunner-server
OTEL_RESOURCE_ATTRIBUTES=deployment.environment=production,service.version=1.0.0
```

**Note**: Telemetry is optional. If OTLP endpoint is not configured, the server will start without metrics/tracing.

## References

### Specifications
- [specs/job_api.md](job_api.md) - Original job API design
- [PHASE2_IMPLEMENTATION_SUMMARY.md](../PHASE2_IMPLEMENTATION_SUMMARY.md) - Phase 2 implementation details

### Core Implementation
- [internal/store/store.go](../internal/store/store.go) - JobStore interface and MemoryJobStore
- [internal/store/sqs_store.go](../internal/store/sqs_store.go) - SQS/DynamoDB implementation
- [internal/worker/worker.go](../internal/worker/worker.go) - Worker job executor with event publishing

### Telemetry & Observability
- [internal/telemetry/telemetry.go](../internal/telemetry/telemetry.go) - OTLP exporter initialization
- [internal/telemetry/metrics.go](../internal/telemetry/metrics.go) - Metric instrument definitions

### Protocol Definitions
- [api/job/v1/job.proto](../api/job/v1/job.proto) - Protobuf definitions for Job API
